{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Installation Setup",
   "id": "e3ffdf9a856ca120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "13a6cad910438889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns; sns.set()\n",
    "from pandas.tseries.offsets import *\n",
    "from dateutil.relativedelta import *\n",
    "import datetime as dt\n",
    "import os\n",
    "from linearmodels.asset_pricing import TradedFactorModel, LinearFactorModel\n",
    "from IPython.core.pylabtools import figsize\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from fredapi import Fred\n",
    "fred = Fred(api_key = 'b0363f9c9d853b92b27e06c4727bc2ea')\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "%matplotlib inline \n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20,10)"
   ],
   "id": "59f10f4de4dd22ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from multiprocessing import Pool \n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "import StockPortfolioEnv\n",
    "import utils\n",
    "import TD3_BC\n",
    "\n",
    "import pytz\n",
    "import itertools\n",
    "from datetime import datetime as dt\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)"
   ],
   "id": "64683d587e913212"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "idx = pd.IndexSlice\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "434283b36ef4cca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Environment configuration\n",
    "> A gym-style portfolio allocation environment for agents to interact. It is handy to compare the performances."
   ],
   "id": "ae930edfed6b685e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = pd.read_csv('data/train_data.csv', index_col=0)\n",
    "trade_data = pd.read_csv('data/trade_data.csv', index_col=0)"
   ],
   "id": "f6040fa3d6446048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train = train_data\n",
    "trade = trade_data\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = stock_dimension\n",
    "tech_indicator_list = ['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    "feature_dimension = len(tech_indicator_list)\n",
    "\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "print(f\"Feature Dimension: {feature_dimension}\")\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"transaction_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-1\n",
    "}\n",
    "\n",
    "e_train_gym = StockPortfolioEnv.StockPortfolioEnv(df = train, **env_kwargs)\n",
    "e_trade_gym = StockPortfolioEnv.StockPortfolioEnv(df = trade, **env_kwargs)"
   ],
   "id": "e0ef321bc8d1f059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retail_train = StockPortfolioEnv.sample_from_env(i=0, env=e_train_gym, weight_type=\"RETAIL\")",
   "id": "7eba756e50821516"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modelling\n",
    "> Use a two-stage scheme (supervised learning & reinforcement learning), in analogy to AlphaGo and ChatGPT. The first stage learns from human trader logs, while the second stage leverages reinforcement learning to achieve super-human performance."
   ],
   "id": "9a7e552368ab508f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_ = StockPortfolioEnv.sample_from_env(i=0, env=e_train_gym, weights=train_data[\"moribvol\"])\n",
    "true_portfolio_train = e_train_gym.asset_memory\n",
    "true_actions_train = e_train_gym.actions_memory"
   ],
   "id": "d2dd04ec58452a6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each row of x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def plot_portfolio(y1, y2):\n",
    "    plt.plot(y1, label='reg')\n",
    "    plt.plot(y2, label='Ground Truth')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.xlabel('timesteps')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_mse(y1, y2):\n",
    "    plt.plot(np.mean((y1 - y2.values.reshape(-1, stock_dimension)), axis=1)**2)\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('timesteps')\n",
    "    plt.show()"
   ],
   "id": "35ccef0c2be8e596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regression",
   "id": "6d292817e9656f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = train_data[tech_indicator_list]\n",
    "Y = train_data[\"moribvol\"]\n",
    "\n",
    "olsres = sm.OLS(Y.values.reshape(-1),\n",
    "                sm.add_constant(X.values.reshape(-1, feature_dimension))).fit()\n",
    "\n",
    "reg_fit = olsres.predict(sm.add_constant(X.values.reshape(-1, feature_dimension))).reshape(-1, stock_dimension)\n",
    "reg_fit = softmax(y_fit)"
   ],
   "id": "ba7ef6ba54fe06a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_ = StockPortfolioEnv.sample_from_env(i=0, env=e_train_gym, weights=reg_fit)\n",
    "reg_portfolio_train = e_train_gym.asset_memory\n",
    "\n",
    "plot_portfolio(reg_portfolio_train, true_portfolio_train)"
   ],
   "id": "226d722f7d6afb83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_mse(reg_fit, Y)",
   "id": "2f6aa348331cea42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "olsres = sm.OLS(pd.Series(true_portfolio_train).pct_change(), \n",
    "                (sm.add_constant(pd.Series(reg_portfolio_train)).pct_change()), missing=\"drop\").fit()\n",
    "\n",
    "print(olsres.summary())\n",
    "\n",
    "# olsres = sm.OLS(np.asarray(true_actions_train).reshape(-1), \n",
    "#                 np.asarray(reg_fit[:-1]).reshape(-1)).fit()\n",
    "\n",
    "# print(olsres.summary())"
   ],
   "id": "33af828dee5ae5ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tree",
   "id": "49b4f51b05fd95a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "clf_fit = clf.predict((X.values.reshape(-1, feature_dimension))).reshape(-1, stock_dimension)\n",
    "clf_fit = softmax(clf_fit)"
   ],
   "id": "afc5d395172c12ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_ = StockPortfolioEnv.sample_from_env(i=0, env=e_train_gym, weights=clf_fit)\n",
    "clf_portfolio_train = e_train_gym.asset_memory\n",
    "\n",
    "plot_portfolio(clf_portfolio_train, true_portfolio_train)"
   ],
   "id": "eac3761e0a7d02a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_mse(clf_fit, Y)",
   "id": "26306a6711271ef9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "olsres = sm.OLS(pd.Series(true_portfolio_train).pct_change(), \n",
    "                (sm.add_constant(pd.Series(reg_portfolio_train)).pct_change()), missing=\"drop\").fit()\n",
    "\n",
    "print(olsres.summary())\n",
    "\n",
    "# olsres = sm.OLS(np.asarray(true_actions_train).reshape(-1), \n",
    "#                 np.asarray(clf_fit[:-1]).reshape(-1)).fit()\n",
    "\n",
    "# print(olsres.summary())"
   ],
   "id": "98bb4961c1fb328b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "48137ee9e9ebb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "      \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return F.softmax(out)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepare dataset\n",
    "X = X.astype(np.float32) \n",
    "Y = Y.astype(np.float32) \n",
    "\n",
    "dataset = Dataset(X.values.reshape(-1, 44), Y.values.reshape(-1, 11))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "lstm = LSTMModel(input_size=44, hidden_size=128, num_layers=1, output_size=11)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "num_epochs = 5000\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "b8e74a1f109141de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lstm_fit = lstm(dataset.X).detach().numpy()\n",
    "_ = StockPortfolioEnv.sample_from_env(i=0, env=e_train_gym, weights=lstm_fit)\n",
    "lstm_portfolio_train = e_train_gym.asset_memory\n",
    "\n",
    "plot_portfolio(reg_portfolio_train, true_portfolio_train)"
   ],
   "id": "2778ed2aab0f0df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_mse(lstm_fit, Y)",
   "id": "d9d944990f03d6c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "olsres = sm.OLS(pd.Series(true_portfolio_train).pct_change(), \n",
    "                (sm.add_constant(pd.Series(lstm_portfolio_train)).pct_change()), missing=\"drop\").fit()\n",
    "\n",
    "print(olsres.summary())"
   ],
   "id": "82fef2bad480efb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## First stage - Supervised Learning\n",
    "> Fundemental models: regression, tree, lstm and ann \n",
    "\n",
    "> Placebo tests: features (subset or random variables that has the mean and std), imitate mean-var, XLF"
   ],
   "id": "419613c7a698f20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Runs policy for X episodes and returns D4RL score: A fixed seed is used for the eval environment\n",
    "\n",
    "def eval_policy(policy, eval_env, seed, mean, std, seed_offset=100, eval_episodes=1):\n",
    "    # eval_env = gym.make(env_name)\n",
    "    eval_env.reset()\n",
    "    eval_env.seed(seed + seed_offset)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            state = (np.array(state).reshape(1,-1) - mean)/std\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    # \td4rl_score = eval_env.get_normalized_score(avg_reward) * 100\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    # \tprint(f\"Evaluation over {eval_episodes} episodes\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ],
   "id": "8a18bfa821ccd00a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment\n",
    "policy = \"TD3+BC\" # Policy name\n",
    "env = e_train_gym # OpenAI gym environment name\n",
    "seed = 0 # Sets Gym, PyTorch and Numpy seeds\n",
    "eval_freq = 1e3 # How often (time steps) we evaluate\n",
    "max_timesteps = 1e5 # Max time steps to run environment\n",
    "save_model = True # Save model and optimizer parameters\n",
    "load_model = \"\" # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "file_name = f\"BC_{seed}\"\n",
    "\n",
    "# TD3\n",
    "expl_noise = 0.1 # Std of Gaussian exploration noise\n",
    "batch_size = 256 # Batch size for both actor and critic\n",
    "discount = 0.99 # Discount factor\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # Noise added to target policy during critic update\n",
    "noise_clip = 0.5 # Range to clip target policy noise\n",
    "policy_freq = 1 # Frequency of delayed policy updates\n",
    "\n",
    "# TD3 + BC\n",
    "alpha = 0\n",
    "normalize = True"
   ],
   "id": "6c36d2ef262ee946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"---------------------------------------\")\n",
    "print(f\"Policy: {policy}, Env: {env}, Seed: {seed}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# Set seeds\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
    "action_dim = env.action_space.shape[0] \n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": discount,\n",
    "    \"tau\": tau,\n",
    "    # TD3\n",
    "    \"policy_noise\": policy_noise * max_action,\n",
    "    \"noise_clip\": noise_clip * max_action,\n",
    "    \"policy_freq\": policy_freq,\n",
    "    # TD3 + BC\n",
    "    \"alpha\": alpha\n",
    "}\n",
    "\n",
    "# Initialize policy\n",
    "policy = TD3_BC.TD3_BC(**kwargs)\n",
    "\n",
    "# if load_model != \"\":\n",
    "#     policy_file = file_name if load_model == \"default\" else load_model\n",
    "#     policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "replay_buffer.convert_D4RL(retail_train)\n",
    "\n",
    "# flatten\n",
    "replay_buffer.state = replay_buffer.state.reshape(replay_buffer.state.shape[0], -1)\n",
    "replay_buffer.next_state = replay_buffer.next_state.reshape(replay_buffer.next_state.shape[0], -1)\n",
    "\n",
    "if normalize:\n",
    "    mean,std = replay_buffer.normalize_states() \n",
    "else:\n",
    "    mean,std = 0,1"
   ],
   "id": "8849e80f68d92cbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "days = len(env.actions_memory)\n",
    "evaluations = []\n",
    "portfolio_values = []\n",
    "fitted = []\n",
    "\n",
    "for t in range(int(max_timesteps)):\n",
    "    policy.train(replay_buffer, batch_size)\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % eval_freq == 0:\n",
    "        print(f\"Time steps: {t+1}\")\n",
    "        evaluations.append(eval_policy(policy, env, seed, mean, std))\n",
    "        portfolio_values.append(env.portfolio_value)\n",
    "        fitted.append(np.mean((np.array(env.actions_memory) - np.array(train_y.loc[:days-1][\"moribvol\"]).reshape(-1, 11)) ** 2))\n",
    "        \n",
    "        # np.save(f\"./results/{file_name}\", evaluations)\n",
    "        # if save_model: policy.save(f\"./models/{file_name}\")"
   ],
   "id": "e001d3fec885da2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Mertrics",
   "id": "f0f65ca2d08c175a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if file_name == f\"BC_{seed}\":\n",
    "    evaluations_0 = evaluations.copy()\n",
    "    portfolio_values_0 = portfolio_values.copy()\n",
    "    fitted_0 = fitted.copy()\n",
    "elif file_name == f\"TD3+BC_{seed}\":\n",
    "    evaluations_1 = evaluations.copy()\n",
    "    portfolio_values_1 = portfolio_values.copy()\n",
    "    fitted_1 = fitted.copy()\n",
    "\n",
    "evaluations_2 = sum(retail_train[\"rewards\"])\n",
    "portfolio_values_2 = sum(retail_train[\"rewards\"]) + 1000000 # 1000000 being initial capital"
   ],
   "id": "d2f16149bf13c542"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cumulative rewards",
   "id": "8e10b15964545ecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), evaluations_0, label='SL')\n",
    "# plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), evaluations_1, label='SL+RL')\n",
    "plt.axhline(y=evaluations_2, label='Ground Truth', color = 'r')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Training')\n",
    "plt.ylabel('cumulative rewards (PnL)')\n",
    "plt.xlabel('timesteps')\n",
    "plt.show()"
   ],
   "id": "db0d503f356a187d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Portfolio values",
   "id": "f5579819b539fd03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), portfolio_values_0, label='SL')\n",
    "# plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), evaluations_1, label='SL+RL')\n",
    "plt.axhline(y=portfolio_values_2, label='Ground Truth', color = 'r')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Training')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.xlabel('timesteps')\n",
    "plt.show()"
   ],
   "id": "9bc8e0caaae992da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MSE",
   "id": "6c63af25c6f883d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), fitted_0, label='SL')\n",
    "# plt.plot(range(int(eval_freq), int(max_timesteps+1), int(eval_freq)), evaluations_1, label='SL+RL')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Training')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('timesteps')\n",
    "plt.show()"
   ],
   "id": "d9901505a4a40cc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Backtest",
   "id": "22f7ad4991f9f89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from copy import deepcopy\n",
    "trade_dataset = sample_from_env(i=0, env=e_trade_gym, is_train=False)\n",
    "true_asset_memory_trade = deepcopy(e_trade_gym.asset_memory)\n",
    "true_action_memory_trade = deepcopy(e_trade_gym.actions_memory)"
   ],
   "id": "2bad10cffddf9fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "replay_buffer.convert_D4RL(trade_dataset)\n",
    "seed = 0\n",
    "\n",
    "# flatten\n",
    "replay_buffer.state = replay_buffer.state.reshape(replay_buffer.state.shape[0], -1)\n",
    "replay_buffer.next_state = replay_buffer.next_state.reshape(replay_buffer.next_state.shape[0], -1)\n",
    "\n",
    "if normalize:\n",
    "    mean,std = replay_buffer.normalize_states() \n",
    "else:\n",
    "    mean,std = 0,1\n",
    "    \n",
    "eval_policy(policy, e_trade_gym, seed, mean, std)"
   ],
   "id": "ef771cc1a3e7ff4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# historical portfolio values \n",
    "plt.plot(trade_y['date'].unique(), e_trade_gym.asset_memory[:-1], label='Predicted')\n",
    "plt.plot(trade_y['date'].unique(), true_asset_memory_trade[:-1], label='Ground Truth')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio values ')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "2ef0678da1cfec62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# how weights change over time (simulated vs. real)\n",
    "# Hard to observe though!\n",
    "\n",
    "selected_tech_tic = [\"QCOM\", \"ADSK\", \"FSLR\", \"MSFT\", \"AMD\", \"ORCL\", \"INTU\", \"WU\", \"LRCX\", \"TXN\", \"CSCO\"]\n",
    "selected_tech_tic.sort()\n",
    "\n",
    "true_action_memory_trade = np.asarray(true_action_memory_trade)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=11, ncols=1, figsize=(8, 20), sharex=True)\n",
    "\n",
    "# Plot each weight column against the date column\n",
    "for i in range(11):\n",
    "    axs[i].plot(trade_y['date'].unique()[:-1], true_action_memory_trade[:, i])\n",
    "    axs[i].set_ylabel(\"Fitted \" + selected_tech_tic[i])\n",
    "    axs[i].grid(True)\n",
    "\n",
    "axs[-1].set_xlabel('Date')\n",
    "plt.suptitle('Weight Changes Over Time', fontsize=16)\n",
    "plt.show()"
   ],
   "id": "1b9a071af32e4c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Measures (OLS, cosine similarity, or correlation) of asset allocations\n",
    "\n",
    "# Compute cosine similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cos_sims = [1 - cosine(true_action_memory_trade[i], e_trade_gym.actions_memory[i]) for i in range(len(true_action_memory_trade))]\n",
    "plt.plot(trade_y['date'].unique()[:-1], cos_sims)\n",
    "plt.xlabel('Date')\n",
    "plt.title('Cosine Similarity')\n",
    "plt.show()\n"
   ],
   "id": "a93c05054b755b9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plt.plot(np.std(true_action_memory_trade, axis=1))\n",
    "np.std(true_action_memory_trade, axis=1)"
   ],
   "id": "8f2df7810bf2b9bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plt.plot(np.std(e_trade_gym.actions_memory, axis=1))\n",
    "np.std(e_trade_gym.actions_memory, axis=1)"
   ],
   "id": "dab3c25cfe7e562d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# correlation\n",
    "corrcoef = [np.corrcoef(true_action_memory_trade[i], e_trade_gym.actions_memory[i])[0, 1]\n",
    "                        for i in range(len(true_action_memory_trade))]\n",
    "plt.plot(trade_y['date'].unique()[:-1], corrcoef)\n",
    "plt.xlabel('Date')\n",
    "plt.title('Correlation')\n",
    "plt.show()"
   ],
   "id": "533c68f86fcbf90a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = [mean_squared_error(true_action_memory_trade[i], e_trade_gym.actions_memory[i])\n",
    "       for i in range(len(true_action_memory_trade))]\n",
    "plt.plot(trade_y['date'].unique()[:-1], mse)\n",
    "plt.xlabel('Date')\n",
    "plt.title('MSE')\n",
    "plt.show()"
   ],
   "id": "1c545ee08c86070a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#fit linear regression model\n",
    "\n",
    "# olsres = sm.OLS(np.asarray(true_action_memory_trade).reshape(-1), \n",
    "#                 np.asarray(e_trade_gym.actions_memory).reshape(-1)).fit()\n",
    "\n",
    "# print(olsres.summary())\n",
    "\n",
    "olsres = sm.OLS(pd.Series(true_asset_memory_trade[:-1]).pct_change(), \n",
    "                sm.add_constant(pd.Series(e_trade_gym.asset_memory[:-1]).pct_change()), missing=\"drop\").fit()\n",
    "\n",
    "# plt.plot(trade_y['date'].unique(), e_trade_gym.asset_memory[:-1], label='Predicted')\n",
    "# plt.plot(trade_y['date'].unique(), true_asset_memory_trade[:-1], label='Ground Truth')\n",
    "\n",
    "#view model summary\n",
    "print(olsres.summary())"
   ],
   "id": "f9316bce9120e44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e66b58ce9336b06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9d9942f3512434c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a26726d69175bd9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exploration to finetune (TD3+BC)",
   "id": "2750bd8e81285f42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Experiment\n",
    "# policy = \"TD3+BC\" # Policy name\n",
    "# env = e_train_gym # OpenAI gym environment name\n",
    "# seed = 0 # Sets Gym, PyTorch and Numpy seeds\n",
    "# eval_freq = 1e3 # How often (time steps) we evaluate\n",
    "# max_timesteps = 1e5 # Max time steps to run environment\n",
    "# save_model = True # Save model and optimizer parameters\n",
    "# load_model = \"\" # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "# file_name = f\"TD3_{seed}\"\n",
    "\n",
    "# # TD3\n",
    "# expl_noise = 0.1 # Std of Gaussian exploration noise\n",
    "# batch_size = 256 # Batch size for both actor and critic\n",
    "# discount = 0.99 # Discount factor\n",
    "# tau = 0.005 # Target network update rate\n",
    "# policy_noise = 0.2 # Noise added to target policy during critic update\n",
    "# noise_clip = 0.5 # Range to clip target policy noise\n",
    "# policy_freq = 2 # Frequency of delayed policy updates\n",
    "\n",
    "# # TD3 + BC\n",
    "# alpha = 0\n",
    "# beta = 1\n",
    "# normalize = True"
   ],
   "id": "60ac22106716b317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Set seeds\n",
    "# env.seed(seed)\n",
    "# env.action_space.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# state_dim = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
    "# action_dim = env.action_space.shape[0] \n",
    "# max_action = float(env.action_space.high[0])"
   ],
   "id": "7b201b9afd731ba9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(\"---------------------------------------\")\n",
    "# print(f\"Policy: {policy}, Env: {env}, Seed: {seed}\")\n",
    "# print(\"---------------------------------------\")\n",
    "\n",
    "# kwargs = {\n",
    "#     \"state_dim\": state_dim,\n",
    "#     \"action_dim\": action_dim,\n",
    "#     \"max_action\": max_action,\n",
    "#     \"discount\": discount,\n",
    "#     \"tau\": tau,\n",
    "#     # TD3\n",
    "#     \"policy_noise\": policy_noise * max_action,\n",
    "#     \"noise_clip\": noise_clip * max_action,\n",
    "#     \"policy_freq\": policy_freq,\n",
    "#     # TD3 + BC\n",
    "#     \"alpha\": alpha,\n",
    "#     \"beta\": beta\n",
    "# }\n",
    "\n",
    "# # Initialize policy\n",
    "# policy = TD3_BC.TD3_BC(**kwargs)\n",
    "# # policy.load(f\"./models/BC_0\")"
   ],
   "id": "34c3632dc3c9859d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "# replay_buffer.convert_D4RL(retail_train)\n",
    "\n",
    "# # flatten\n",
    "# replay_buffer.state = replay_buffer.state.reshape(replay_buffer.state.shape[0], -1)\n",
    "# replay_buffer.next_state = replay_buffer.next_state.reshape(replay_buffer.next_state.shape[0], -1)\n",
    "\n",
    "# if normalize:\n",
    "#     mean,std = replay_buffer.normalize_states() \n",
    "# else:\n",
    "#     mean,std = 0,1"
   ],
   "id": "afb4fc670d918c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# evaluations = []\n",
    "# portfolio_values = []\n",
    "# for t in range(int(max_timesteps)):\n",
    "#     policy.train(replay_buffer, batch_size)\n",
    "#     # Evaluate episode\n",
    "#     if (t + 1) % eval_freq == 0:\n",
    "#         print(f\"Time steps: {t+1}\")\n",
    "#         evaluations.append(eval_policy(policy, env, seed, mean, std))\n",
    "#         portfolio_values.append(env.portfolio_value)\n"
   ],
   "id": "ec5adfe67c89a1f5"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
